{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REFORMAT DATASET\n",
    "Reformatting the layout of the overall dataset in terms of column information, tupled statsitical data, and identifying the null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year:  2007 , Data:  59010\n",
      "Year:  2008 , Data:  71190\n",
      "Year:  2009 , Data:  57398\n",
      "Year:  2010 , Data:  62807\n",
      "Year:  2011 , Data:  79091\n",
      "Year:  2012 , Data:  64503\n",
      "Year:  2013 , Data:  59986\n",
      "Year:  2014 , Data:  59476\n",
      "Year:  2015 , Data:  57906\n",
      "Year:  2016 , Data:  56005\n",
      "Year:  2017 , Data:  57029\n",
      "Year:  2018 , Data:  62693\n",
      "Year:  2019 , Data:  67859\n",
      "Year:  2020 , Data:  61237\n",
      "Year:  2021 , Data:  61243\n",
      "Year:  2022 , Data:  68703\n"
     ]
    }
   ],
   "source": [
    "##Importing Libraries For Exploration\n",
    "import pandas as pd\n",
    "\n",
    "NaturalDisaster_Data = pd.read_csv(\"./Storm Dataset/StormDetails2007.csv\")\n",
    "\n",
    "#Iterate through all the Storm Details years and combine datasets to create one single large one\n",
    "for year in range(2007, 2023):\n",
    "    currentYear_Data = pd.read_csv(f\"./Storm Dataset/StormDetails{year}.csv\")\n",
    "    #NaturalDisaster_Data.append(currentYear_Data, ignore_index = True)\n",
    "    NaturalDisaster_Data = pd.concat([NaturalDisaster_Data, currentYear_Data])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BEGIN_YEARMONTH', 'BEGIN_DAY', 'BEGIN_TIME', 'END_YEARMONTH',\n",
       "       'END_DAY', 'END_TIME', 'EPISODE_ID', 'EVENT_ID', 'STATE',\n",
       "       'STATE_FIPS', 'YEAR', 'MONTH_NAME', 'EVENT_TYPE', 'CZ_TYPE',\n",
       "       'CZ_FIPS', 'CZ_NAME', 'WFO', 'BEGIN_DATE_TIME', 'CZ_TIMEZONE',\n",
       "       'END_DATE_TIME', 'INJURIES_DIRECT', 'INJURIES_INDIRECT',\n",
       "       'DEATHS_DIRECT', 'DEATHS_INDIRECT', 'DAMAGE_PROPERTY',\n",
       "       'DAMAGE_CROPS', 'SOURCE', 'MAGNITUDE', 'MAGNITUDE_TYPE',\n",
       "       'FLOOD_CAUSE', 'CATEGORY', 'TOR_F_SCALE', 'TOR_LENGTH',\n",
       "       'TOR_WIDTH', 'TOR_OTHER_WFO', 'TOR_OTHER_CZ_STATE',\n",
       "       'TOR_OTHER_CZ_FIPS', 'TOR_OTHER_CZ_NAME', 'BEGIN_RANGE',\n",
       "       'BEGIN_AZIMUTH', 'BEGIN_LOCATION', 'END_RANGE', 'END_AZIMUTH',\n",
       "       'END_LOCATION', 'BEGIN_LAT', 'BEGIN_LON', 'END_LAT', 'END_LON',\n",
       "       'EPISODE_NARRATIVE', 'EVENT_NARRATIVE', 'DATA_SOURCE'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NaturalDisaster_Data.columns.values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA PREPROCESSING\n",
    "Cleaning up the data in terms of normalization, feature selection, and ordinally categorizing the the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5e3958ac43a9925cccec3cc0b90475e9a1eb582b0a026a9f6f10e7e3d12170fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
